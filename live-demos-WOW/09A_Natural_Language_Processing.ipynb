{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc546052",
   "metadata": {},
   "source": [
    "# Natural Language Processing: an introduction\n",
    "\n",
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "\n",
    "%precision 4\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"../datasets/IMDB-Movie-Data.csv\")\n",
    "movies_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e97a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_corpus = movies_df[\"Description\"].tolist()\n",
    "movies_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec59ad",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# download puntuation resources for NLTK\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(movies_corpus[0])\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(doc) for doc in movies_corpus]\n",
    "print(tokenized_corpus[100:103])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e74e8",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer\n",
    "\n",
    "A better tokenizer to remove also all the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82307142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "tokens = [tok for tok in word_tokenize(movies_corpus[0]) if tok not in punctuation]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(movies_corpus[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = list(map(word_tokenize, movies_corpus))\n",
    "print(tokenized_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cd82a",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6bfae7",
   "metadata": {},
   "source": [
    "You can already see that there is something potentially dangerous happening here. \"Will\" is a stopwords but as a token\n",
    "it can mean many things. In a sentence such as \"My will is strong.\", \"will\" is not an auxiliary and shoud not be considered a stopword.\n",
    "\n",
    "We'll ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(movies_corpus[0])\n",
    " \n",
    "filtered_sentence = [tok for tok in word_tokens if not tok.lower() in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8be2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def remove_stopwords(doc_tokens: List[str]) -> List[str]:\n",
    "    return [tok for tok in doc_tokens if not tok.lower() in stop_words]\n",
    "\n",
    "remove_stopwords(word_tokenize(movies_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a06a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenized_corpus = [remove_stopwords(tokens) for tokens in tokenized_corpus]\n",
    "print(filtered_tokenized_corpus[100:103])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be017978",
   "metadata": {},
   "source": [
    "### Lemmatization with Part-Of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e3e73",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging\n",
    "\n",
    "Part of Speech tagging means labelling words in a sentence as nouns, adjectives, verbs...etc. NLTK can also label by tense, count, and more. Here's a list of the tags, what they mean, and some examples:\n",
    "\n",
    "```\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent\\'s\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-of-speech tagging\n",
    "sentence_pos = pos_tag(filtered_tokenized_corpus[0])\n",
    "print(sentence_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991e72",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word.\n",
    "\n",
    "The `WordNetLemmatizer` relies on the WordNet lexical database of English: https://wordnet.princeton.edu/ \n",
    "\n",
    "One major difference with stemming is that lemmatize takes a part of speech parameter, “pos” If not supplied, the default is “noun.”\n",
    "\n",
    "In the context of the NLTK Lemmatization, the part of speech tags are pre-defined with shortcuts for the NLTK `WordNetLemmatizer` as below.\n",
    "\n",
    "```\n",
    "ADJ = \"a\"\n",
    "ADJ_SAT = \"s\"\n",
    "ADV = \"r\" \n",
    "NOUN = \"n\" \n",
    "VERB = \"v\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ea25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_doc = []\n",
    "\n",
    "def get_lemmatizer_pos(nltk_tag: str):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:          \n",
    "            return wordnet.NOUN\n",
    "\n",
    "for token, tag in sentence_pos:\n",
    "    # get the part-of-speech\n",
    "        pos = get_lemmatizer_pos(tag)\n",
    "\n",
    "        lemmatized_doc.append(lemmatizer.lemmatize(token, pos))\n",
    "print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545eb09a",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentPreProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self._lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _remove_stopwords(doc_tokens: List[str]) -> List[str]:\n",
    "        return [tok for tok in doc_tokens if not tok.lower() in stop_words]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lemmatizer_pos(nltk_tag: str):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        if nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        if nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        if nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV          \n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    def preprocess(self, doc: str) -> List[str]:\n",
    "        # tokenize\n",
    "        tokenized_doc: List[str] = self._tokenizer.tokenize(doc)# remove stop words (optional)\n",
    "        # unstopped_doc: str = self._remove_stopwords(doc)\n",
    "        # print(unstopped_doc)\n",
    "        # remove stop words (optional)\n",
    "        filtered_tokens: str = self._remove_stopwords(tokenized_doc)\n",
    "        # print(filtered_tokens)\n",
    "        lemmatized_doc: List[str] = []\n",
    "        for token, tag in pos_tag(filtered_tokens):\n",
    "            pos = self._get_lemmatizer_pos(tag)\n",
    "            \n",
    "            # lemmatize based on POS\n",
    "            lemmatized_doc.append(self._lemmatizer.lemmatize(token, pos))\n",
    "            \n",
    "        return lemmatized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = DocumentPreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor.preprocess(movies_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_corpus = list(map(pre_processor.preprocess, movies_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4539b0",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "Now we need to create a corpora Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the dictionary on your corpus\n",
    "dictionary = corpora.Dictionary(pre_processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_corpus_bow = [dictionary.doc2bow(doc_tokens) for doc_tokens in pre_processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies_corpus_bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f57c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(dictionary[idx], bow_count) for idx, bow_count in movies_corpus_bow[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8208fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2f7bd",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Formula (from redacteur.com)\n",
    "\n",
    "<img src=\"https://www.redacteur.com/blog/wp-content/uploads/2020/02/tf-idf-2-1-1024x375-1.png\" alt=\"Drawing\" style=\"height: 200px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel(movies_corpus_bow) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f524af",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(dictionary[idx], tfidf_score) for idx, tfidf_score in tfidf_model[movies_corpus_bow[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c55f3",
   "metadata": {},
   "source": [
    "Let's now apply our TF-IDF model to our whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64221f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_corpus_tfidf = tfidf_model[movies_corpus_bow]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093cccc",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. \n",
    "\n",
    "LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ec3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(movies_corpus_tfidf, id2word=dictionary, num_topics=9)\n",
    "movies_corpus_lsi = lsi_model[movies_corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ea4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4901b25",
   "metadata": {},
   "source": [
    "#### Display the result of Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ab791",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top words per topic\n",
    "num_words = 10\n",
    "\n",
    "topic_words = pd.DataFrame()\n",
    "\n",
    "for idx, topic in enumerate(lsi_model.get_topics()):\n",
    "    # get the token ids of the \"num_words\" top words per topic\n",
    "    # NB: numpy.argsort() returns the indices that would sort an array.\n",
    "    top_feature_ids = topic.argsort()[-num_words:][::-1]\n",
    "    # get the values corresponding to each word/token\n",
    "    feature_values = topic[top_feature_ids]\n",
    "    # get the actual token/word out of its id\n",
    "    words = [dictionary[id_] for id_ in top_feature_ids]\n",
    "    # put topic index, word/token, and value into a DataFrame\n",
    "    topic_df = pd.DataFrame({\n",
    "        \"value\": feature_values,\n",
    "        \"word\": words,\n",
    "        \"topic\": idx\n",
    "    })\n",
    "    topic_words = pd.concat([topic_words, topic_df], ignore_index=True)\n",
    "\n",
    "topic_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "graph = sns.FacetGrid(topic_words, col=\"topic\", col_wrap=3, sharey=False)\n",
    "_ = graph.map(plt.barh, \"word\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17585870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision 2\n",
    "print(movies_corpus_lsi[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
